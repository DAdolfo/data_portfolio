{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e07f2c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from itertools import chain\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/Applications/Spark\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk/Contents/Home\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4593be1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.logger import PySparkLogger\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, FloatType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c315a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "{\"ts\":\"2025-09-29T18:19:19.715Z\",\"level\":\"WARN\",\"msg\":\"Your hostname, jaguilar.local, resolves to a loopback address: 127.0.0.1; using 192.168.1.5 instead (on interface en0)\",\"logger\":\"Utils\"}\n",
      "{\"ts\":\"2025-09-29T18:19:19.718Z\",\"level\":\"WARN\",\"msg\":\"Set SPARK_LOCAL_IP if you need to bind to another address\",\"logger\":\"Utils\"}\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "{\"ts\":\"2025-09-29T18:19:20.388Z\",\"level\":\"WARN\",\"msg\":\"Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\",\"logger\":\"NativeCodeLoader\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 54676)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/Users/jaguilar/Desktop/Portfolio/data_pipeline/venv/lib/python3.11/site-packages/pyspark/accumulators.py\", line 299, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/Users/jaguilar/Desktop/Portfolio/data_pipeline/venv/lib/python3.11/site-packages/pyspark/accumulators.py\", line 271, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/Users/jaguilar/Desktop/Portfolio/data_pipeline/venv/lib/python3.11/site-packages/pyspark/accumulators.py\", line 275, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jaguilar/Desktop/Portfolio/data_pipeline/venv/lib/python3.11/site-packages/pyspark/serializers.py\", line 597, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "#spark.sparkContext.setLogLevel(\"WARN\") #To keep the notebook clean for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd48882a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = PySparkLogger.getLogger(\"spark_logger\")\n",
    "handler = logging.FileHandler(\"cleanup.log\")\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798f9a29",
   "metadata": {},
   "source": [
    "### BigQuery Raw Data Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2c5032",
   "metadata": {},
   "source": [
    "First, we've gotta do a clean up of the \"databases\" the business already has in place, as if their data platform team does a good job of maintaining it in proper form :sunglasses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4b29d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2025-09-29 20:19:26.790\", \"level\": \"INFO\", \"logger\": \"spark_logger\", \"msg\": \"Users Database has 100000 records as of this pipeline run.\", \"context\": {\"users_size\": 100000}}\n",
      "{\"ts\": \"2025-09-29 20:19:26.791\", \"level\": \"INFO\", \"logger\": \"spark_logger\", \"msg\": \"Orders Database has 125188 records as of this pipeline run.\", \"context\": {\"orders_size\": 125188}}\n",
      "{\"ts\": \"2025-09-29 20:19:26.792\", \"level\": \"INFO\", \"logger\": \"spark_logger\", \"msg\": \"Products Database has 29120 records as of this pipeline run.\", \"context\": {\"products_size\": 29120}}\n"
     ]
    }
   ],
   "source": [
    "users = spark.read.csv(\"../tables/users_table.csv\", header=True, inferSchema=True)\n",
    "orders = spark.read.csv(\"../tables/orders_table.csv\", header=True, inferSchema=True)\n",
    "products = spark.read.csv(\"../tables/products_table.csv\", header=True, inferSchema=True)\n",
    "\n",
    "users_size = users.count()\n",
    "orders_size = orders.count()\n",
    "products_size = products.count()\n",
    "\n",
    "logger.info(f\"Users Database has {users_size} records as of this pipeline run.\", users_size=users_size)\n",
    "logger.info(f\"Orders Database has {orders_size} records as of this pipeline run.\", orders_size=orders_size)\n",
    "logger.info(f\"Products Database has {products_size} records as of this pipeline run.\", products_size=products_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57f98d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2025-09-29 20:19:31.473\", \"level\": \"INFO\", \"logger\": \"spark_logger\", \"msg\": \"The users database has the following amount of null values: {'id': 0, 'first_name': 0, 'last_name': 0, 'email': 0, 'age': 0, 'gender': 0, 'state': 0, 'street_address': 0, 'postal_code': 0, 'city': 0, 'country': 0, 'latitude': 0, 'longitude': 0, 'traffic_source': 0, 'created_at': 0, 'user_geom': 0}\", \"context\": {\"users_null_dictionary\": {\"id\": 0, \"first_name\": 0, \"last_name\": 0, \"email\": 0, \"age\": 0, \"gender\": 0, \"state\": 0, \"street_address\": 0, \"postal_code\": 0, \"city\": 0, \"country\": 0, \"latitude\": 0, \"longitude\": 0, \"traffic_source\": 0, \"created_at\": 0, \"user_geom\": 0}}}\n",
      "{\"ts\": \"2025-09-29 20:19:32.449\", \"level\": \"INFO\", \"logger\": \"spark_logger\", \"msg\": \"The orders database has the following amount of null values: {'order_id': 0, 'user_id': 0, 'status': 0, 'gender': 0, 'created_at': 0, 'returned_at': 112841, 'shipped_at': 43812, 'delivered_at': 81271, 'num_of_item': 0}\", \"context\": {\"orders_null_dictionary\": {\"order_id\": 0, \"user_id\": 0, \"status\": 0, \"gender\": 0, \"created_at\": 0, \"returned_at\": 112841, \"shipped_at\": 43812, \"delivered_at\": 81271, \"num_of_item\": 0}}}\n",
      "{\"ts\": \"2025-09-29 20:19:33.229\", \"level\": \"INFO\", \"logger\": \"spark_logger\", \"msg\": \"The products database has the following amount of null values: {'id': 0, 'cost': 0, 'category': 0, 'name': 0, 'brand': 24, 'retail_price': 0, 'department': 0, 'sku': 0, 'distribution_center_id': 0}\", \"context\": {\"products_null_dictionary\": {\"id\": 0, \"cost\": 0, \"category\": 0, \"name\": 0, \"brand\": 24, \"retail_price\": 0, \"department\": 0, \"sku\": 0, \"distribution_center_id\": 0}}}\n"
     ]
    }
   ],
   "source": [
    "users_null_dictionary = {col : users.filter(users[col].isNull()).count() for col in users.columns}\n",
    "logger.info(f\"The users database has the following amount of null values: {users_null_dictionary}\",\n",
    "    users_null_dictionary=users_null_dictionary)\n",
    "\n",
    "orders_null_dictionary = {col : orders.filter(orders[col].isNull()).count() for col in orders.columns}\n",
    "logger.info(f\"The orders database has the following amount of null values: {orders_null_dictionary}\",\n",
    "    orders_null_dictionary=orders_null_dictionary)\n",
    "\n",
    "products_null_dictionary = {col : products.filter(products[col].isNull()).count() for col in products.columns}\n",
    "logger.info(f\"The products database has the following amount of null values: {products_null_dictionary}\",\n",
    "    products_null_dictionary=products_null_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ca4b9d",
   "metadata": {},
   "source": [
    "- We can see that the users database is good to go :white_check_mark:\n",
    "- Orders have nulls for values in columns that recorded an instance, such as `returned_at, shipped_at (worrying), delivered_at`, and so on. We'll have to check if the nulls make sense :eyes:\n",
    "- Twenty-four of the products don't have a brand, this affects analytics and would be manually handled in real life, so we'll name them and give them a good brand name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8018d53",
   "metadata": {},
   "source": [
    "## Orders Cleanup\n",
    "\n",
    "All orders have values of `created_at`, that's good :thumbsup:.\\\n",
    " After this we have to make sure they're shipped, then they should be delivered and lastly returned.\n",
    "\n",
    "1. With `shipped_at` having 81,376 non-null values, we should expect delivered orders to be the same or smaller.\n",
    "2. We have 43,917 `delivered_at` orders in total.\n",
    "3. And finally we have 12,347 orders that have values in the column `returned_at`. \n",
    "\n",
    "As a business this would be an alarming rate of delivered/shipped orders but as a portfolio project, this is more than interesting to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9edcc282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders.select(F.col(\"order_id\")).where((F.col(\"shipped_at\").isNull()) & (F.col(\"delivered_at\").isNotNull())).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0894e016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders.select(F.col(\"order_id\")).where((F.col(\"delivered_at\").isNull()) & (F.col(\"returned_at\").isNotNull())).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c9ef28",
   "metadata": {},
   "source": [
    "Now that we're sure the way those columns were filled makes sense, we can continue with the products table:\n",
    "\n",
    "## Products Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b79280e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+--------------------+--------------------+-----+------------------+----------+--------------------+----------------------+\n",
      "|   id|              cost|            category|                name|brand|      retail_price|department|                 sku|distribution_center_id|\n",
      "+-----+------------------+--------------------+--------------------+-----+------------------+----------+--------------------+----------------------+\n",
      "|  755| 15.14085034638792|         Tops & Tees|The Very Hungry C...| NULL|28.950000762939453|     Women|CCB0989662211F61E...|                     3|\n",
      "| 1629| 23.57235048930124|Fashion Hoodies &...|Carhartt Women's ...| NULL|45.950000762939446|     Women|5C50B4DF4B176845C...|                     3|\n",
      "| 8600| 16.01555072412528|   Outerwear & Coats|Women's Micro Fle...| NULL|  35.9900016784668|     Women|CE840AA9583592E71...|                     3|\n",
      "| 9482|5.7119999527931204|     Socks & Hosiery|KEEN Women Bellin...| NULL|              16.0|     Women|C5A3C867A3DFB7765...|                     3|\n",
      "|10598| 9.616399956984818|           Intimates|JMS Comfort Lace ...| NULL|16.579999923706055|     Women|E6FA05C07B144B6FF...|                     3|\n",
      "|11389|12.511999994516373|           Intimates|Shadowline 36 Fla...| NULL|              23.0|     Women|47C6D232C6A446811...|                     3|\n",
      "|11668| 18.66240072498321|           Intimates|Everyday Comfort ...| NULL|38.400001525878906|     Women|67E4423AFD0FFD2F4...|                     3|\n",
      "|11843| 33.89999996870756|           Intimates|Wendy Glez Rose C...| NULL|              60.0|     Women|B451DA363BB08B9A8...|                     3|\n",
      "|13863| 9.554499962599948|         Accessories|Husky Animal Hat ...| NULL|             24.25|     Women|7992308D89FF6B43B...|                     3|\n",
      "|13921|1.6419000095449388|         Accessories|NEW Aluminum Cred...| NULL| 4.210000038146973|     Women|0CBFF6DD75DF1F43E...|                     3|\n",
      "|15723| 4.875119887263626|                Plus|Vintage Wayfarer ...| NULL| 9.989999771118164|     Women|67F19FD0431CAC0AC...|                     3|\n",
      "|15757|2.2355100181230902|                Plus|NEW Aluminum Cred...| NULL| 4.210000038146973|     Women|05482621678342824...|                     3|\n",
      "|16309|11.519999988377094|         Tops & Tees|Hurley Men's One ...| NULL|              20.0|       Men|88E0F16114A1E011C...|                     3|\n",
      "|16559|14.944249824823812|         Tops & Tees|Gildan Adult Ultr...| NULL|25.989999771118164|       Men|DC0FC2D90D6102BA0...|                     3|\n",
      "|16898|13.924999965820462|         Tops & Tees|Quiksilver Waterm...| NULL|              25.0|       Men|22811EE1984621751...|                     3|\n",
      "|21207|30.754350295802205|               Jeans|Ariat 9424 Men's ...| NULL|59.950000762939446|       Men|A59687B426E9EB304...|                     3|\n",
      "|21484|30.285499976947904|               Jeans|True Nation Big &...| NULL|              59.5|       Men|64C32B8EB5304D80B...|                     3|\n",
      "|23769|18.540000021457672|   Outerwear & Coats|Stormtech Men's C...| NULL|              45.0|       Men|1C51851B7A12EED5C...|                     3|\n",
      "|24287| 39.48804877663827|   Outerwear & Coats|Adidas Men's Hiki...| NULL| 89.94999694824217|       Men|9F62C81D7B48E1464...|                     3|\n",
      "|25135|  12.4763997392416|               Socks|Volcom Men's Argy...| NULL|22.479999542236328|       Men|0DC76D68D436BB030...|                     3|\n",
      "+-----+------------------+--------------------+--------------------+-----+------------------+----------+--------------------+----------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "products.select(\"*\").where(F.col(\"brand\").isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a15eba",
   "metadata": {},
   "source": [
    "As filling in individual rows with different values proves almost impossible with Spark (because honestly, just do it in the database, this tool is made to handle a lot of data at the same time), we'll fill in the missing brands with my country's _\"We make everything for cheap\"_ brand: **Suli**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a315d000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2025-09-29 20:20:05.013\", \"level\": \"INFO\", \"logger\": \"spark_logger\", \"msg\": \"The products database has been cleared of null values, by filling in the missing data.\", \"context\": {}}\n"
     ]
    }
   ],
   "source": [
    "products = products.fillna(value=\"Suli\")\n",
    "logger.info(\"The products database has been cleared of null values, by filling in the missing data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61027f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2025-09-29 20:20:09.287\", \"level\": \"INFO\", \"logger\": \"spark_logger\", \"msg\": \"The products database has the following amount of null values: {'id': 0, 'cost': 0, 'category': 0, 'name': 0, 'brand': 0, 'retail_price': 0, 'department': 0, 'sku': 0, 'distribution_center_id': 0}\", \"context\": {\"products_null_dictionary\": {\"id\": 0, \"cost\": 0, \"category\": 0, \"name\": 0, \"brand\": 0, \"retail_price\": 0, \"department\": 0, \"sku\": 0, \"distribution_center_id\": 0}}}\n"
     ]
    }
   ],
   "source": [
    "products_null_dictionary = {col : products.filter(products[col].isNull()).count() for col in products.columns}\n",
    "logger.info(f\"The products database has the following amount of null values: {products_null_dictionary}\",\n",
    "    products_null_dictionary=products_null_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d84c255",
   "metadata": {},
   "source": [
    "Now we can continue with the simulated daily batch of orders we'd get as a business."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cbcddd",
   "metadata": {},
   "source": [
    "### Simulated Batches Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a5fc635",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"sequence_number\", IntegerType(), True),\n",
    "    StructField(\"session_id\", StringType(), True),\n",
    "    StructField(\"created_at\", TimestampType(), True),\n",
    "    StructField(\"ip_address\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"postal_code\", StringType(), True),\n",
    "    StructField(\"browser\", StringType(), True),\n",
    "    StructField(\"traffic_source\", StringType(), True),\n",
    "    StructField(\"uri\", StringType(), True),\n",
    "    StructField(\"event_type\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a61f857f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2025-09-29 20:22:17.289\", \"level\": \"INFO\", \"logger\": \"spark_logger\", \"msg\": \"Events data loaded, there are 1200000 records on it.\", \"context\": {\"events_size\": 1200000}}\n"
     ]
    }
   ],
   "source": [
    "events = spark.read.csv(\"../tables/events_table.csv\", header=True, schema=events_schema).persist()\n",
    "events_size = events.count()\n",
    "logger.info(f\"Events data loaded, there are {events_size} records on it.\", events_size=events_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3300da31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2025-09-29 20:22:19.508\", \"level\": \"INFO\", \"logger\": \"spark_logger\", \"msg\": \"The events data has the following amount of null values: {'id': 0, 'user_id': 561396, 'sequence_number': 0, 'session_id': 0, 'created_at': 0, 'ip_address': 0, 'city': 0, 'state': 0, 'postal_code': 0, 'browser': 0, 'traffic_source': 0, 'uri': 0, 'event_type': 0}\", \"context\": {\"Events_null_dictionary\": {\"id\": 0, \"user_id\": 561396, \"sequence_number\": 0, \"session_id\": 0, \"created_at\": 0, \"ip_address\": 0, \"city\": 0, \"state\": 0, \"postal_code\": 0, \"browser\": 0, \"traffic_source\": 0, \"uri\": 0, \"event_type\": 0}}}\n"
     ]
    }
   ],
   "source": [
    "#Let's check what columns have nulls and how many of them\n",
    "Events_null_dictionary = {col : events.filter(events[col].isNull()).count() for col in events.columns}\n",
    "logger.info(f\"The events data has the following amount of null values: {Events_null_dictionary}\",\n",
    "     Events_null_dictionary=Events_null_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebac23f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------------+------------------------------------+-------------------+---------------+-------+--------+-----------+-------+--------------+-------+----------+\n",
      "|id     |user_id|sequence_number|session_id                          |created_at         |ip_address     |city   |state   |postal_code|browser|traffic_source|uri    |event_type|\n",
      "+-------+-------+---------------+------------------------------------+-------------------+---------------+-------+--------+-----------+-------+--------------+-------+----------+\n",
      "|2302471|NULL   |3              |6552bb55-8b9e-4eb5-bd1e-3f7f67ca168d|2019-04-02 11:33:00|172.175.102.165|Sapporo|Hokkaido|005-0849   |IE     |Adwords       |/cancel|cancel    |\n",
      "+-------+-------+---------------+------------------------------------+-------------------+---------------+-------+--------+-----------+-------+--------------+-------+----------+\n",
      "only showing top 1 row\n"
     ]
    }
   ],
   "source": [
    "#Sadly we know that the events locaiton columns city, state and country are not discrete enough to let us narrow down \n",
    "#a user for its mapping\n",
    "#Now we should check whether the columns contains values that make sense, such as status, gender and num of item.\n",
    "#Columns as dates are checked when loading the CSV, they'd be nulls if they didn't match the format \n",
    "#(Spark's default reading mode)\n",
    "events.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02002f9d",
   "metadata": {},
   "source": [
    "Come to think of it, maybe the user_id is null due to all of the subsequent rows being part of one session, and thus traceable to one user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed124ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------------+------------------------------------+-------------------+---------------+-------+--------+-----------+-------+--------------+--------------+----------+\n",
      "|id     |user_id|sequence_number|session_id                          |created_at         |ip_address     |city   |state   |postal_code|browser|traffic_source|uri           |event_type|\n",
      "+-------+-------+---------------+------------------------------------+-------------------+---------------+-------+--------+-----------+-------+--------------+--------------+----------+\n",
      "|2302469|NULL   |1              |6552bb55-8b9e-4eb5-bd1e-3f7f67ca168d|2019-04-02 11:15:00|172.175.102.165|Sapporo|Hokkaido|005-0849   |IE     |Adwords       |/product/16284|product   |\n",
      "|2302470|NULL   |2              |6552bb55-8b9e-4eb5-bd1e-3f7f67ca168d|2019-04-02 11:27:00|172.175.102.165|Sapporo|Hokkaido|005-0849   |IE     |Adwords       |/cart         |cart      |\n",
      "|2302471|NULL   |3              |6552bb55-8b9e-4eb5-bd1e-3f7f67ca168d|2019-04-02 11:33:00|172.175.102.165|Sapporo|Hokkaido|005-0849   |IE     |Adwords       |/cancel       |cancel    |\n",
      "+-------+-------+---------------+------------------------------------+-------------------+---------------+-------+--------+-----------+-------+--------------+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "events.select(\"*\").where(F.col(\"session_id\") == \"6552bb55-8b9e-4eb5-bd1e-3f7f67ca168d\").orderBy(\"sequence_number\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da72c2bf",
   "metadata": {},
   "source": [
    "Well it isn't the way I suspected it could be, but then we can generate fake_ids to have the totality of the data for the Churn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d6d182e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sessionids_with_null_userids = list(events.select(\"session_id\").distinct().where(F.col(\"user_id\").isNull()).toPandas()[\"session_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b4052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if any user_id is null for a session that does have a user_id record\n",
    "# cantnameit = list(events.select(\"id\")\n",
    "#         .where(\n",
    "#             (F.col(\"session_id\").isin(sessionids_with_null_userids)) &\n",
    "#             (F.col(\"user_id\").isNotNull())\n",
    "#         )\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3204f22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if sessionids_with_null_userids:\n",
    "#     #The whole ass query thing and the filling process\n",
    "#     print(\"hola\")\n",
    "# elif cantnameit:\n",
    "#     #drop the rows with the ids in cantnameit\n",
    "#     print(\"adios\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e888e874",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events.createOrReplaceTempView(\"events_table\")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    AVG(number_of_sessions) as avg_sessions_per_user\n",
    "FROM\n",
    "    (SELECT\n",
    "        user_id,\n",
    "        COUNT(DISTINCT(session_id)) as number_of_sessions\n",
    "    FROM\n",
    "        events_table\n",
    "    WHERE\n",
    "        user_id IS NOT NULL\n",
    "    GROUP BY\n",
    "        user_id\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "avg_sessions_per_user = int(round(spark.sql(query).first()[0],0))\n",
    "\n",
    "avg_sessions_per_user\n",
    "\n",
    "#Don't use CTE in Spark, better to push filters first in the FROM clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7cbf41e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_ids = []\n",
    "value = 1\n",
    "fake_ids_amount = math.floor(len(sessionids_with_null_userids)/avg_sessions_per_user)\n",
    "for i in range(fake_ids_amount):\n",
    "    fake_ids.append(f\"fake_id_{value}_{datetime.now().strftime('%Y-%m-%d')}\") #These are daily batches\n",
    "    value += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "573f36aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\":\"2025-09-29T18:23:54.981Z\",\"level\":\"WARN\",\"msg\":\"No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\",\"logger\":\"WindowExec\"}\n",
      "{\"ts\":\"2025-09-29T18:23:54.981Z\",\"level\":\"WARN\",\"msg\":\"No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\",\"logger\":\"WindowExec\"}\n",
      "{\"ts\":\"2025-09-29T18:23:54.981Z\",\"level\":\"WARN\",\"msg\":\"No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\",\"logger\":\"WindowExec\"}\n"
     ]
    }
   ],
   "source": [
    "df1 = events.filter(F.col(\"user_id\").isNull()).select(\"session_id\")\n",
    "\n",
    "window = Window.orderBy(\"session_id\")\n",
    "\n",
    "#Given that the window function is over the whole dataset, we persist it\n",
    "df1 = df1.withColumn(\"row_number\", F.dense_rank().over(window)).persist() \n",
    "df1 = df1.withColumn(\"fake_index\", (((F.col(\"row_number\") - 1)/avg_sessions_per_user).cast(\"int\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbcdf43",
   "metadata": {},
   "source": [
    "There would be no data loss if we generate fake_ids for the missing sessions. \n",
    "We'll fill them with the average amount of sessions a user has had historically per batch (this batch in our project's case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a118b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_ids_df = spark.createDataFrame([(i, fake_ids[i]) for i in range(len(fake_ids))], [\"fake_index\", \"fake_id\"])\n",
    "\n",
    "filled_df = df1.join(fake_ids_df, on=\"fake_index\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4a897b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\":\"2025-09-29T18:25:03.464Z\",\"level\":\"WARN\",\"msg\":\"No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\",\"logger\":\"WindowExec\"}\n",
      "{\"ts\":\"2025-09-29T18:25:03.465Z\",\"level\":\"WARN\",\"msg\":\"No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\",\"logger\":\"WindowExec\"}\n",
      "{\"ts\":\"2025-09-29T18:25:03.644Z\",\"level\":\"WARN\",\"msg\":\"No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\",\"logger\":\"WindowExec\"}\n",
      "{\"ts\":\"2025-09-29T18:25:03.644Z\",\"level\":\"WARN\",\"msg\":\"No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\",\"logger\":\"WindowExec\"}\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------------+---------------+-------------------+---------------+------------+--------------------+-----------+-------+--------------+--------------------+----------+\n",
      "|          session_id|     id|             user_id|sequence_number|         created_at|     ip_address|        city|               state|postal_code|browser|traffic_source|                 uri|event_type|\n",
      "+--------------------+-------+--------------------+---------------+-------------------+---------------+------------+--------------------+-----------+-------+--------------+--------------------+----------+\n",
      "|09c046f5-d079-45f...|1153971|               88494|              7|2020-09-04 04:01:27|  75.232.73.112|    Petaluma|          California|      94952| Safari|         Email|/department/men/c...|department|\n",
      "|19298899-38b0-45d...|1861162|fake_id_18433_202...|              3|2022-03-22 14:39:00|110.148.245.212|     Beijing|Xinjiang Uygur Au...|     841001| Chrome|         Email|               /cart|      cart|\n",
      "|19298899-38b0-45d...|1861162|fake_id_18433_202...|              3|2022-03-22 14:39:00|110.148.245.212|     Beijing|Xinjiang Uygur Au...|     841001| Chrome|         Email|               /cart|      cart|\n",
      "|1d6235dd-b536-4da...|1548419|fake_id_21483_202...|              1|2022-06-02 09:20:00|  61.216.96.102|     Tianjin|               Henan|     450002| Chrome|         Email|      /product/21392|   product|\n",
      "|257fffb1-60e1-4fa...|1736858|fake_id_27360_202...|              1|2023-08-09 15:51:00|   49.81.155.42|      Málaga|           Andalucía|      29014| Safari|         Email|      /product/20253|   product|\n",
      "|306515bd-1e7f-4d9...|1979126|fake_id_35358_202...|              3|2024-09-01 03:20:00|  70.142.151.61|South Jordan|                Utah|      84095| Chrome|         Email|               /cart|      cart|\n",
      "|340a18de-683f-4f4...|1671310|fake_id_38018_202...|              2|2024-11-27 18:39:00|165.242.141.192|     Quimper|            Bretagne|      29000| Chrome|         Email|      /product/20796|   product|\n",
      "|340a18de-683f-4f4...|1671310|fake_id_38018_202...|              2|2024-11-27 18:39:00|165.242.141.192|     Quimper|            Bretagne|      29000| Chrome|         Email|      /product/20796|   product|\n",
      "|367d0ef0-b83f-463...|1721241|fake_id_39822_202...|              1|2021-12-26 02:17:00|   173.76.187.1|Philadelphia|        Pennsylvania|      19148| Chrome|       Adwords|/department/men/c...|department|\n",
      "|367d0ef0-b83f-463...|1721241|fake_id_39822_202...|              1|2021-12-26 02:17:00|   173.76.187.1|Philadelphia|        Pennsylvania|      19148| Chrome|       Adwords|/department/men/c...|department|\n",
      "|367d0ef0-b83f-463...|1721241|fake_id_39822_202...|              1|2021-12-26 02:17:00|   173.76.187.1|Philadelphia|        Pennsylvania|      19148| Chrome|       Adwords|/department/men/c...|department|\n",
      "|375ea926-6941-4a5...|2129357|fake_id_40479_202...|              3|2021-06-21 17:58:00| 96.217.223.223|   São Paulo|           São Paulo|  02675-031|Firefox|         Email|             /cancel|    cancel|\n",
      "|375ea926-6941-4a5...|2129357|fake_id_40479_202...|              3|2021-06-21 17:58:00| 96.217.223.223|   São Paulo|           São Paulo|  02675-031|Firefox|         Email|             /cancel|    cancel|\n",
      "|3f0808c9-cb9b-42d...|1263190|               96867|              2|2023-10-22 02:37:11| 216.43.233.118|    Richmond|          California|      94801|Firefox|       Adwords|/department/men/c...|department|\n",
      "|570707cc-8faf-4b7...| 858734|               65687|              4|2021-12-11 05:30:31| 214.245.24.228|        Napa|          California|      94558| Chrome|         Email|/department/women...|department|\n",
      "|6210a912-ef41-465...|2281420|fake_id_71796_202...|              3|2023-09-24 14:13:00|   40.67.176.84|   São Paulo|           São Paulo|  02675-031| Safari|       YouTube|             /cancel|    cancel|\n",
      "|77a2765b-e7aa-48a...|1020103|               78247|             11|2024-10-01 15:53:54| 214.255.160.17|      Merced|          California|      95340| Chrome|         Email|      /product/26062|   product|\n",
      "|77a2765b-e7aa-48a...|1020097|               78247|              5|2024-10-01 15:46:16| 214.255.160.17|      Merced|          California|      95340| Chrome|         Email|      /product/26062|   product|\n",
      "|7fb956b3-7f55-4fb...| 781286|               59677|              5|2024-09-10 08:10:58|102.102.114.158| Los Angeles|          California|      90023| Chrome|         Email|           /purchase|  purchase|\n",
      "|88a6a627-c616-401...|1559653|fake_id_100064_20...|              1|2021-03-09 15:42:00| 163.108.129.15|Philadelphia|        Pennsylvania|      19148|     IE|         Email|/department/women...|department|\n",
      "+--------------------+-------+--------------------+---------------+-------------------+---------------+------------+--------------------+-----------+-------+--------------+--------------------+----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "events = (events.join(filled_df.select(\"session_id\", \"fake_id\"),\n",
    "                        on=\"session_id\", how=\"left\").withColumn(\"user_id\", F.coalesce(\"user_id\", filled_df[\"fake_id\"]))\n",
    "                .drop(\"fake_id\")\n",
    ")\n",
    "events.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42ac4048",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2025-09-29 20:25:33.838\", \"level\": \"INFO\", \"logger\": \"spark_logger\", \"msg\": \"The events data has the following amount of null values: {'session_id': 0, 'id': 0, 'user_id': 0, 'sequence_number': 0, 'created_at': 0, 'ip_address': 0, 'city': 0, 'state': 0, 'postal_code': 0, 'browser': 0, 'traffic_source': 0, 'uri': 0, 'event_type': 0}\", \"context\": {\"Events_null_dictionary\": {\"session_id\": 0, \"id\": 0, \"user_id\": 0, \"sequence_number\": 0, \"created_at\": 0, \"ip_address\": 0, \"city\": 0, \"state\": 0, \"postal_code\": 0, \"browser\": 0, \"traffic_source\": 0, \"uri\": 0, \"event_type\": 0}}}\n"
     ]
    }
   ],
   "source": [
    "Events_null_dictionary = {col : events.filter(events[col].isNull()).count() for col in events.columns}\n",
    "logger.info(f\"The events data has the following amount of null values: {Events_null_dictionary}\",\n",
    "     Events_null_dictionary=Events_null_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c527377",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = events.drop_duplicates(subset=[\"id\", \"user_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ae278c",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_sources = [\"Organic\", \"YouTube\", \"Email\", \"Adwords\", \"Facebook\"]\n",
    "event_types = [\"cancel\", \"purchase\", \"cart\", \"cart\", \"department\", \"home\", \"product\"]\n",
    "\n",
    "events = events.filter(F.col(\"traffic_source\").isin(traffic_sources))\n",
    "events = events.filter(F.col(\"event_type\").isin(event_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "146e97c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\":\"2025-09-29T18:35:26.821Z\",\"level\":\"WARN\",\"msg\":\"Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\\nScaling row group sizes to 95.00% for 8 writers\",\"context\":{\"task_name\":\"task 0.0 in stage 373.0 (TID 798)\"},\"logger\":\"MemoryManager\"}\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "events.write.parquet(f'./cleaned_data/cleaned_events_batch_{datetime.now().strftime(\"%Y-%m-%d\")}',\n",
    "                     mode = \"overwrite\",\n",
    "                     partitionBy=\"event_type\" #To be defined once we know what to train the model with\n",
    "                     \n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec83a86",
   "metadata": {},
   "source": [
    "## What's next? Analytics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b8c9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_items_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"inventory_item_id\", IntegerType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"created_at\", TimestampType(), True),\n",
    "    StructField(\"shipped_at\", TimestampType(), True),\n",
    "    StructField(\"delivered_at\", TimestampType(), True),\n",
    "    StructField(\"returned_at\", TimestampType(), True),\n",
    "    StructField(\"sale_price\", FloatType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b871b349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
